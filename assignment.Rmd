---
title: "Practical Machine Learning assignment"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

Various accelerometer measurements (about 50 features remain after preprocessing of about 160 total features) are used to predict the outcome of a weight lifting excercise. The outcomes are classified according to their "correctness" of how the excercise movements have been performed.  There are 5 outcome classes, of roughly equal ocurrence, so no skewness in stratification needs to be taken into account. 

A random forest approach is chosen for its robustness, with k-fold cross validation to counter the tendency of decision trees to be overfitted. Over 99% accuracy is achieved (vs just 50% on a single decison tree).

## Preprocessing
After loading the data, removing timestamps, row index, columns with near zero variance, and columns with over 95% NA values:  

```{r housekeeping, echo=TRUE}
library(ggplot2)
library(lattice)
library(caret)
library(rattle)

dRaw     <- read.csv("../pml-training.csv")
testing  <- read.csv("../pml-testing.csv")

# removing row number (X) and timestamps (not doing time series analyses here)
rc <- c("X","user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp")
rc1 <- which(colnames(dRaw) %in% rc)
dclean <- dRaw[,-rc1]

# removing features with near zero variance
rc2 <- nearZeroVar(dclean)
dclean <- dclean[,-rc2]

# removing features with mostly (95%) NA (not addressed by nearZeroVar)
rc3 <- which(colSums(is.na(dclean)) / nrow(dclean) > 0.95)
dclean <- dclean[,-rc3]

set.seed(12345)
intrain <- createDataPartition(dclean$classe, p = .70, list = FALSE)
dtrain <- dclean[intrain, ]
dtest  <- dclean[-intrain, ]
```

This leaves about 50 features for the classification task. The data set contains about 20'000 records, a 70/30 train / test split is chosen, so that there is sufficient amount of test data to estimate the classifier accuracy.

## Analyses

Random Forest is chosen as classification approach as it is relatively robust with respect to outliers and correlated features.  As an example, a single decision tree is trained and plotted below, achieving about 50% accuracy: 
```{r}
tree <- train(classe ~., data=dtrain, method="rpart")
fancyRpartPlot(tree$finalModel, caption="Tree trained with RPart")
tree
```

Moving on to a Randon Forest, here using 5-fold cross-validation (being mindful of computation time):
```{r RandomForest, echo=TRUE}
param <- trainControl(method="cv", 5)
mod <- train(classe ~ ., data=dtrain, method="rf", trControl=param, ntree=200)
mod
```

As can be seen from the table above, the accuracy of the optimal model is 99.69%.  Given the large data set and k-fold cross-validation, we should expect a similar outcome with the final test set. And indeed, as can be seen below with the test set the accuracy is even slightly higher at 99.88%, and the lower 95% confidence interval boundary on the accuracy is 99.76%.   

```{r}
pred <- predict(mod, dtest)
confusionMatrix(dtest$classe, pred)
```


```{r eval=FALSE, include=FALSE}
predict(mod, testing)
```

